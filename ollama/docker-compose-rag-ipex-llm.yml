services:
  ollama-rag-llm-model-ipex-llm:
    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
    container_name: ollama-rag-llm-model-ipex-llm
    hostname: ollama-rag-llm-model-ipex-llm
    environment:
      - OLLAMA_HOST="0.0.0.0:11434"
      - MODEL=${OLLAMA_LLM_MODEL}
      - DEVICE=Arc
      - OLLAMA_INTEL_GPU=true
      - OLLAMA_NUM_GPU=999
      - ZES_ENABLE_SYSMAN=1
      - ONEAPI_DEVICE_SELECTOR=level_zero:0
    #      - GGML_SYCL_DISABLE_OPT=0
    #      - GGML_SYCL_DISABLE_GRAPH=0
    #      - GGML_SYCL_F16=1
    tty: true
    privileged: true
    devices:
      - /dev/dri
    volumes:
      - "${OLLAMA_DATA}:/root/.ollama"
    command: sh -c 'mkdir -p /llm/ollama && cd /llm/ollama && init-ollama && exec ./ollama serve'

  ollama-rag-embed-model-ipex-llm:
    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
    container_name: ollama-rag-embed-model-ipex-llm
    hostname: ollama-rag-embed-model-ipex-llm
    environment:
      - OLLAMA_HOST="0.0.0.0:11434"
      - MODEL=${OLLAMA_EMBED_MODEL}
      - DEVICE=Arc
      - OLLAMA_INTEL_GPU=true
      - OLLAMA_NUM_GPU=999
      - ZES_ENABLE_SYSMAN=1
      - ONEAPI_DEVICE_SELECTOR=level_zero:1
    #      - GGML_SYCL_DISABLE_OPT=0
    #      - GGML_SYCL_DISABLE_GRAPH=0
    #      - GGML_SYCL_F16=1
    tty: true
    privileged: true
    devices:
      - /dev/dri
    volumes:
      - "${OLLAMA_DATA}:/root/.ollama"
    command: sh -c 'mkdir -p /llm/ollama && cd /llm/ollama && init-ollama && exec ./ollama serve'

  ollama-rag-qdrant:
    image: qdrant/qdrant
    container_name: ollama-rag-qdrant
    hostname: ollama-rag-qdrant
    volumes:
      - "${QDRANT_DATA}:/qdrant/storage"

  ollama-rag-anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: ollama-rag-anythingllm
    hostname: ollama-rag-anythingllm
    depends_on:
      - ollama-rag-llm-model-ipex-llm
      - ollama-rag-embed-model-ipex-llm
      - ollama-rag-qdrant
    cap_add:
      - SYS_ADMIN
    environment:
      - LLM_PROVIDER=ollama
      - EMBEDDING_MODEL_PREF=nomic-embed-text:latest
      - OLLAMA_BASE_PATH=http://ollama-rag-llm-model-ipex-llm:11434
      - OLLAMA_MODEL_PREF=llama3.1:8b
      - OLLAMA_MODEL_TOKEN_LIMIT=4096
      - OLLAMA_PERFORMANCE_MODE=base
      - OLLAMA_KEEP_ALIVE_TIMEOUT=300
      - EMBEDDING_ENGINE=ollama
      - EMBEDDING_BASE_PATH=http://ollama-rag-embed-model-ipex-llm:11434
      - EMBEDDING_MODEL_MAX_CHUNK_LENGTH=8192
      - VECTOR_DB=qdrant
      - QDRANT_ENDPOINT=http://ollama-rag-qdrant:6333
      - STORAGE_DIR=/app/server/storage
      - SERVER_PORT=${HOST_PORT}
    ports:
      - "${HOST_IP}:${HOST_PORT}:${HOST_PORT}"
    volumes:
      - "${ANYLLM_DATA}/storage:/app/server/storage"
      - "${ANYLLM_DATA}/.env:/app/server/.env"
